<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Decision Tree | In-Between</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Decision Tree" />
<meta name="author" content="Yang An" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Let’s start from an example, the table below lists the factors that affect people’s decision to play golf." />
<meta property="og:description" content="Let’s start from an example, the table below lists the factors that affect people’s decision to play golf." />
<link rel="canonical" href="http://localhost:4001/math/2021/03/25/Decision-Tree.html" />
<meta property="og:url" content="http://localhost:4001/math/2021/03/25/Decision-Tree.html" />
<meta property="og:site_name" content="In-Between" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-25T08:20:46+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Decision Tree" />
<script type="application/ld+json">
{"headline":"Decision Tree","dateModified":"2021-03-25T08:20:46+08:00","datePublished":"2021-03-25T08:20:46+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4001/math/2021/03/25/Decision-Tree.html"},"author":{"@type":"Person","name":"Yang An"},"url":"http://localhost:4001/math/2021/03/25/Decision-Tree.html","@type":"BlogPosting","description":"Let’s start from an example, the table below lists the factors that affect people’s decision to play golf.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4001/feed.xml" title="In-Between" /></head>
<body><!-- <header class="header">
<div class="navigation">

<a href="/" class="logo"></a>

<ul class="menu">
<li class="menu__entry"><a href="/">post</a></li>
<li class="menu__entry"><a href="/projects">projects</a></li> -->
<!-- <li class="menu__entry"><a href="/posts">posts</a></li>
</ul>
</div>
<ul class="social-links">



</ul>
</header> -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\(","\\)"] ],
      },
      TeX: {
        Macros: {
          bra: ["\\langle{#1}|", 1],
          ket: ["|{#1}\\rangle", 1],
          braket: ["\\langle{#1}\\rangle", 1],
          bk: ["\\langle{#1}|{#2}|{#3}\\rangle", 3]
       }
     }
    });
  </script>
  
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML' async></script><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Decision Tree</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2021-03-25T08:20:46+08:00" itemprop="datePublished">Mar 25, 2021
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Yang An</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Let’s start from an example, the table below lists the factors that affect people’s decision to play golf.</p>

<p><img src="/image/Golf_dataset.png" alt="Golf_dataset" /></p>

<p>From the table, it is not straightforward how people make the decision. However, the Decision Tree algorithm can give the graph below and I believe it is much clear to you.</p>

<p><img src="/image/Decision_tree_model.png" alt="Decision_tree" /></p>

<p>Yes, that is Decision Tree. The idea is natural, as it is the same as one of human thinking process. Decision trees are commonly used to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning to do classification.</p>

<p>The mystery I guess is that how each branch is decided. Let’s check the mathematics and algorithm behind. In mathematics part, we will introduce <strong>Impurity</strong> concept and in algorithm part, you can see how impurity is applied to achieve the goal.</p>

<h2 id="mathematics-behind">Mathematics Behind</h2>

<p><strong>Impurity</strong></p>

<blockquote>
  <p>Impurity is a measure of the homogeneity of the labels, in other words, determines how well a decision tree splits.</p>
</blockquote>

<p><strong>Entropy</strong></p>

<blockquote>
  <p>Entropy is a measure of chaos in the dataset, with weighted entropy to evaluate the splits.</p>
</blockquote>

<h2 id="algorithm">Algorithm</h2>

<p>CART (Classification and Regression Trees) — This makes use of Gini impurity as the metric.
ID3 (Iterative Dichotomiser 3) — This uses entropy and information gain as metric.</p>

<p>Gini impurity is defined as</p>

<p>$Impurity = 1 - Gini = 1 - \sum_i{P_i^2}$</p>

<p>where $P_i$ is the probability of one category in the group/subgroup.</p>

<p>Group:</p>
<ul>
  <li>Play: 9, Prob: 9/14</li>
  <li>Don’t Play: 5, Prob: 5/14</li>
  <li>Gini = $(9/14)^2+ (5/14)^2$</li>
  <li>Impurity = 1 - Gini = 1- 0.54 = 0.46</li>
</ul>

<p>Subgroup (weighted impurity):</p>
<ul>
  <li>Outlook = 5/14* Sunny Impurity + 4/14* Overcast Impurity + 5/14 * Rain Impurity = 5/14 * 0.48 + 4/14 * 0 + 5/14 * 0.48 = 0.34</li>
  <li>Windy = 6/14 * Windy Impurity + 8/14 * Non-windy Impurity = 6/14 * 0.38 + 8/14 * 0.5 = 0.43</li>
</ul>

<p>When it comes to continuous variables, the technique of regression tree is used in the algorithm, like CART, ID3. The regression tree determins threshold(s) by the same measure of impurity. There are different methods to determine the threshold(s). More details can be found in the paper <em>Efficient Determination of Dynamic Split Points in a Decision Tree</em> by Microsoft Research.</p>
<ul>
  <li>If the threshold is 85, Humidity = 0.44</li>
  <li>If the threshold is 80, Humidity = 0.39</li>
  <li>If the threshold is 70, Humidity = 0.45</li>
</ul>

<p>Similar regression tree can be done on the feature Temperature. However, you will find the feature Outlook mostly reduces the impurity. Thus it is chosen as the first top leaf on the tree. Similar steps will produce the children leaves.</p>

<h2 id="local-optimum">Local optimum</h2>

<p>It is easy to note that the locally optimal decisions are made at each leaf in the above algorithms. This also makes the decision tree is not robust, i.e. a samll change in the training data can result in a large change in the tree. In this case, you might consider other algorithms such as Dual Information Distance (DID).</p>

  </div><a class="u-url" href="/math/2021/03/25/Decision-Tree.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">In-Between</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">In-Between</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A journey of recognition, appreciation and creation. This blog focuses on translating mathematical concepts  into natural language. It helps understand where the concept comes from, what it exactly means and how we apply.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
